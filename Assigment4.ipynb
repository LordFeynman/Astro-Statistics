{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02202481",
   "metadata": {},
   "source": [
    "# Assignment 4: The bias coin reloaded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09960b6e",
   "metadata": {},
   "source": [
    "Camacho Ronaldo 2051119, Casara Letizia 2063144, Girardi Giorgia 2058916, Quaini Alessandro 2038510, Rosa Valentina 2048656"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9931741a",
   "metadata": {},
   "source": [
    "Suppose that we have a coin, and we would like to figure out what the probability is that it will flip up heads with frequency f."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "004af911",
   "metadata": {},
   "source": [
    "# Q1. Frequentist approach\n",
    "The Binomial distribution is a suitable likelihood function for this problem. Derive an estimator of the bias maximizing the likelihood function (Maximum Likelihood method).  \n",
    "a) Generate N=100 coin flips with an input bias f of your choice. What is the estimated value of f and error bars?  \n",
    "b) Given N=5 toss and 5 heads as outcome, what is the estimated value of f and error bars?  \n",
    "c) Given the condition in a and then b, try hypothesis testing with H0:the coin is not biased vs H1: the coin is biased"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6560b566",
   "metadata": {},
   "source": [
    "# Q2. Bayesian approach\n",
    "The Binomial distribution is a suitable \n",
    "likelihood function for this problem. Use as \"conjugate prior\" the Beta Distribution. Derive an estimator of the bias maximizing \n",
    "the posterior function (Maximum A Posteriori method).  \n",
    "a) Generate N=100 coin flips with an input bias f of your choice. What is the estimated value of f and error bars?   \n",
    "b) Given N=5 toss and 5 heads as outcome what is the estimated value of f and error bars?  \n",
    "c) Model comparison given condition a and b  \n",
    "d) What happens to the MAP estimator when N -> infinity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a2a056f",
   "metadata": {},
   "source": [
    "If we toss a coin N times and the difference coin flips are indipendent, we expect the probability to obtain K heads to be given by a binomial distribution:  \n",
    "$$ P(data|f)=\\binom{N}{K}f^K(1-f)^{N-K}$$\n",
    "which is our likelihood. We know that any smooth unimodal distribution in the interval [0,1] is likely to be approximated by a beta distribution, so it is possible to approximate a Bayesian prior distribution in this way and since the likelihood is a binomial the posterior will be a beta distribution too. \n",
    "\n",
    "$$P(f|\\alpha)=\\frac{\\Gamma(2\\alpha)}{\\Gamma(\\alpha)^2}f^{\\alpha-1}(1-f)^{\\alpha-1} $$\n",
    "    \n",
    "If we want to be agnostic about the coin fairness we would choose an uniform prior, i.e. a beta function with $\\alpha=1$, then the posterior will be the binomial distribution again. $ P(f|data)=\\binom{N}{K}f^K(1-f)^{N-K}$ for $f \\in [0,1]$  \n",
    "Since the posterios associates all different possible values of the parameter to their probability, we can obtain a good estimate by maximizing the posterior itself throught the conditions:\n",
    "$$\\frac{dP}{df}\\bigg|_{f=f_0}=0 \\quad \\mbox{and} \\quad \\frac{d^2P}{df^2}\\bigg|_{f=f_0}<0$$  \n",
    "So, we take the log-posterior $L=logP= const + Klogf+(N-K)log(1-f)$ and we differentiate:  \n",
    "\n",
    "$$ \\frac{dL}{df}=\\frac{K}{f}-\\frac{N-K}{1-f} \\qquad \\frac{d^2L}{df^2}=-\\frac{K}{f^2}-\\frac{N-K}{(1-f)^2}=-\\frac{N}{f(1-f)} $$   \n",
    "The optimal value for f is given by:\n",
    "$$\\frac{dL}{df}\\bigg|_{f=f_0}=\\frac{K}{f_0}-\\frac{N-K}{1-f_0}=0 \\Rightarrow f_0=\\frac{K}{N} $$\n",
    "\n",
    "The error bars are given by the following:\n",
    "\n",
    "$$ \\sigma = \\biggl(- \\frac{d^2L}{df^2}\\bigg|_{f=f_0} \\biggl)^{-1/2}=\\sqrt{\\frac{f_0(1-f_0)}{N}}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e36e00e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K = 46\n",
      "The best estimator for f is f0 = 0.46 +- 0.05\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "#I have assumed that head and tail are equally probable, i.e. f=0.5\n",
    "N=100\n",
    "testa=0\n",
    "count=0\n",
    "while (count<N):\n",
    "    thrown=random.randint(0,1)\n",
    "    if (thrown==0):\n",
    "        testa+=1\n",
    "    count+=1\n",
    "print(\"K =\",testa)\n",
    "f0=testa/N\n",
    "sigma=round(np.sqrt(f0*(1-f0)/N),2)\n",
    "print(\"The best estimator for f is f0 =\",f0, \"+-\", sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2263879",
   "metadata": {},
   "source": [
    "# Q4. The seven scientists. \n",
    "$N$  datapoints {$x_n$} are drawn from $N$ distributions, all of which are Gaussian with a common mean $\\mu$ but with different unknown standard deviations σn. What are the maximum likelihood parameters $\\mu$, {$\\sigma_N$}  given the data? For example, seven scientists (A, B, C, D, E, F, G) with wildly-differing experimental skills measure $\\mu$. You expect some of them to do accurate work (i.e., to have small $\\sigma_n$), and some of them to turn in wildly inaccurate answers (i.e., to have enormous $\\sigma_n$). Figure 22.9 shows their seven results. What is $\\mu$, and how reliable is each scientist? I hope you agree that, intuitively, it looks pretty certain that A and B are both inept measurers, that D–G are better, and that the true value of $\\mu$ is somewhere close to 10. But what does maximizing the likelihood tell you?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66548482",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437e84df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
